\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{titling}
\newlength{\bibitemsep}\setlength{\bibitemsep}{.2\baselineskip plus .05\baselineskip minus .05\baselineskip}
\newlength{\bibparskip}\setlength{\bibparskip}{0pt}
\let\oldthebibliography\thebibliography
\renewcommand\thebibliography[1]{%
  \oldthebibliography{#1}%
  \setlength{\parskip}{\bibitemsep}%
  \setlength{\itemsep}{\bibparskip}%
}



\setlength{\droptitle}{-1cm}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{*0}{*0}
\titlespacing{\subsection}{0pt}{*0}{*0}
\titlespacing{\subsubsection}{0pt}{*0}{*0}


\title{Socio-technical Factors for Automated Test Generation \vspace{-2ex}}


\author{
  Iftekhar Ahmed\\
  University of California, Irvine\\
  \texttt{iftekha@uci.edu} \vspace{-5ex}
  \and
  Alex Groce\\
  Northern Arizona University\\
  \texttt{agroce@gmail.com}\vspace{-5ex}
}

\date{}

\begin{document}
\maketitle

\section{Proposal}

\subsection{Abstract}

This project proposes to investigate the applicability of \emph{socio-technical} factors as automated test generation heuristics. In the first phase of this project, we will investigate the applicability of various \emph{socio-technical} factors: e.g., statements involved in merge conflicts, statements emitting specific bad code smells.   Such factors have not been used in automated test generation, although they have been proven to have significant impact on the overall quality of code, as measured in terms of proneness to bugs. In the second phase, comparing the effectiveness of different \emph{socio-technical} factors as test case generation heuristics along with traditionally used purely technical code factors will also help to identify and evaluate the most suitable ways to generate effective test cases within a limited time budget more suitable for incremental testing.

\subsection{Problem Statement}
Testing is an invaluable technique in ensuring that software is robust and reliable. However, there has been an exponential growth in the complexity of software and the cost of testing also has risen accordingly \cite{myers2011art}. We cannot exhaustively test these complex systems using manually written test case, so one of the ways to ensure quality lies in improved automated test case generation \cite{anand2013orchestrated,harman2012search}. Various properties have been used for generating test cases including
structural \cite{tonella2004evolutionary}, functional \cite{wegener2004evaluation} and non-functional properties \cite{wegener1998verifying}. Majority of these properties are technical in nature, however, software development is not a purely technical activity, it is a complex \emph{socio-technical} activity typically occurring concurrently and within the larger organizational goals and context. Development activity traces left behind in the code base, version control systems, issue trackers, and discussion forums allow us to understand these complex interactions. Researchers have recently started analyzing the complex interactions between \emph{socio-technical} factors for predicting failures but the applicability of these factors in automated test case generation is yet to be investigated. In our own work on identifying \emph{socio-technical} factors with fault prediction capability, we found that merge conflicts and design issues aka "code smells" are better predictors of bugginess of the lines of code \cite{ahmedempirical} when used together rather than being used individually. The primary idea behind this project is that \emph{socio-technical} factors have high association with fault proneness, so automated test cases generated using them , in conjunction with other traditional factors, should provide more effective test cases with  higher defect identification capability.

\subsection{Research Plan}

\noindent {\bf \emph{Socio-technical} factor for test case generation:} Previous work on generating test cases investigated structural \cite{tonella2004evolutionary}, functional \cite{wegener2004evaluation}, non-functional \cite{wegener1998verifying} properties %\cite{oh2011transition}
.Of these studies, none investigated the effectiveness of \emph{socio-technical} factors for test case generation. We propose to produce the first general, systematic examination of various \emph{socio-technical} factors for test case generation. We will start by using a large number of open source source projects collected from sources such as Github and after filtering for the mature open source projects, we will generate tests using state-of-the-art automated test generation systems. Then we will generate tests for the same projects using \emph{socio-technical} factors and compare their effectiveness in identifying real faults. We will use existing benchmarks such as Defects4J \cite{just2014defects4j} and in case of absence of such benchmarks, we will curate our own bug data-set combining information from  sources such as bug tracking systems and patches submitted to the code base for fixing bugs \cite{ahmed2016can}. 
% identified using machine learning techniques \cite{ahmed2016can}. 
We will also use the existing manually written tests and augment them by generating additional tests using both state-of-the-art automated test generation systems and \emph{socio-technical} factors in conjunction and separately and compare their performance to identify the best technique.

\noindent {\bf \emph{Socio-technical} factors for \emph{limited budget} test case generation:} Depending on the results of the first part of the project, it may be possible to better tune automated test generation systems using only \emph{socio-technical} factors, or only traditional factors or a combination of them that works well for limited time budget. It's important to investigate limited time budget test generation techniques that are only proportional to the \emph{size of change} instead of the \emph{whole code base} because as the projects are growing bigger and more complex testing the whole project in a limited time budget is becoming more and more difficult. Testing with a limited budget is also critical for testing in settings such as Travis CI, where many submodules of a large project may need to be tested, and the entire process, including downloading and building the code, is limited to 50 minutes (for private repos) or 120 minutes (for public repos) \cite{TravisDoc}. Researchers have been investigating lightweight automated test generation for a while \cite{groce2012lightweight}. Similar to traditional automated test generation, the methods investigating on the fly automatic testing focuses on using traditional criteria such as code coverage which are technical in nature. Moreover,testing methods that use coverage information face an inherent limitation. For even a perfect method with capabilities for partitioning system behavior by faults, if the method has a cost (over that of random testing), it will be less effective than random testing, for some test budgets \cite{AutoEfficiency}. Small-budget automated test generation, therefore needs more methods that improve on pure random testing but require no or minimal additional computational effort. Ideally, such methods should be able, like random testing, to work even without code coverage or other complex infrastructure. We hypothesize that if \emph{socio-technical} factors in isolation or in combination are effective in automated test case generation, then they can be used for generation of tests for a limited time budget as they require minimal computational effort.

%QuickCheck \cite{claessen2011quickcheck} and other property based testing tools \cite{papadakis2011proper} offer on the fly automatic testing using random testing \cite{bohme2014efficiency}. 
 

%However, testing methods that use coverage information, or even more expensive (and powerful) tools such as symbolic execution \cite{cadar2008klee} face an inherent limitation. For even a perfect method with capabilities for partitioning system behavior by faults, if the method has a cost (over that of random testing), it will be less effective than random testing, for some test budgets \cite{AutoEfficiency}. 

%Real-world techniques are not perfect in their defect targeting, and often impose considerable costs this is why performing symbolic execution only on seed tests, generated by some other method, is now a popular approach in both standard automated test generation and security-based fuzzing \cite{marinescu2012make}. 




\noindent{\bf Synergy:}  The proposed project fits well in the context our our group's ongoing work on using \emph{socio-technical} factors for fault prediction.

%\section{Data Policy}

%All analysis data from this project based on open source projects will be made public, along with analysis scripts, via GitHub or similar open source hosting solution.  Tools for mitigation will be hosted in the same way.  Depending on size of artifacts, only pointers to the actual repository artifact snapshots analyzed may be posted, rather than their full contents (since we expect to potentially analyze very large amounts of source code that is already accessible online).

\bibliographystyle{abbrv}
\bibliography{proposal}

\end{document}
